{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c34feb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27948ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Age = 'junior,middle,senior,senior,middle,junior,junior,middle,middle,junior,junior,senior,middle,junior,junior,senior,senior'.split(',')\n",
    "Married = 'yes,no,no,no,yes,no,yes,yes,no,no,no,yes,yes,no,no,yes,yes'.split(',')\n",
    "Salary = 'high,low,low,low,high,high,low,high,low,low,high,low,high,high,high,high,high'.split(',')\n",
    "Home_owner = 'yes,yes,no,yes,yes,yes,yes,no,no,no,yes,no,yes,yes,no,yes,no'.split(',')\n",
    "Loan_worthy = 'yes,no,no,no,yes,yes,yes,yes,no,no,no,yes,yes,yes,no,yes,yes'.split(',')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Age\"] = Age\n",
    "df[\"Married\"] = Married\n",
    "df['Salary'] = Salary\n",
    "df[\"Home_owner\"] = Home_owner\n",
    "df['Loan_worthy'] = Loan_worthy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4e082",
   "metadata": {},
   "source": [
    "## Entropy of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95570575",
   "metadata": {},
   "source": [
    "Entropy of a dataset is calculated by:\n",
    "$$E = -p\\times log_{2}(p)$$\n",
    "\n",
    "Where **p** is the **probability of occurence** of a particular label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c0d4067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977417817528171"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcualte_entropy_of_dataset(df):\n",
    "    target = df.keys()[-1]\n",
    "    \n",
    "    n_classes,class_counts = np.unique(df[target],return_counts=True)\n",
    "    \n",
    "    p = class_counts/np.sum(class_counts)\n",
    "    \n",
    "    entropy = -p*np.log2(p+eps)\n",
    "    \n",
    "    return np.sum(entropy)\n",
    "\n",
    "\n",
    "calcualte_entropy_of_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8e139",
   "metadata": {},
   "source": [
    "## Gini index of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b95f8",
   "metadata": {},
   "source": [
    "Gini index of the dataset is calculated by the following:\n",
    "$$G = \\sum p*(1 - p)$$\n",
    "\n",
    "Where **p** is the **probability of occurence** of a particular label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe0f5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4844290657439446"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gini_of_dataset(df):\n",
    "    target = df.keys()[-1]\n",
    "    \n",
    "    n_classes,class_counts = np.unique(df[target],return_counts=True)\n",
    "    \n",
    "    p = class_counts/np.sum(class_counts)\n",
    "    \n",
    "    gini = p*(1-p)\n",
    "    \n",
    "    return np.sum(gini)\n",
    "\n",
    "\n",
    "gini_of_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b916b24",
   "metadata": {},
   "source": [
    "## Entropy of an attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1108c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age,0.976829582163908\n",
      "Married,0.4045788563869161\n",
      "Salary,0.7800661722847075\n",
      "Home_owner,0.9240885849733339\n"
     ]
    }
   ],
   "source": [
    "def entropy_of_attribute(df,attribute):\n",
    "    target = df.keys()[-1]\n",
    "    \n",
    "    values_in_target = np.unique(df[target])\n",
    "    \n",
    "    values_in_attribute = np.unique(df[attribute])\n",
    "    \n",
    "    attribute_entropy = 0\n",
    "    for value_of_attribute in values_in_attribute:\n",
    "        overall_entropy = 0\n",
    "        for value_of_target in values_in_target:\n",
    "            num = len(df[attribute][df[attribute] == value_of_attribute][df[target]==value_of_target])\n",
    "            den = len(df[attribute][df[attribute]==value_of_attribute])\n",
    "            \n",
    "            p = num/(den+eps)\n",
    "            \n",
    "            overall_entropy += -p*np.log2(p+eps)\n",
    "            \n",
    "        p2 = den/len(df)\n",
    "        \n",
    "        attribute_entropy += overall_entropy*p2\n",
    "        \n",
    "    return abs(attribute_entropy)\n",
    "\n",
    "for att in df.keys()[:-1]:\n",
    "    print(f'{att},{entropy_of_attribute(df,att)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7dc8be",
   "metadata": {},
   "source": [
    "## Best attribute of divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bfb6a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age,0.0005882353642630012\n",
      "Married,0.572838961141255\n",
      "Salary,0.1973516452434635\n",
      "Home_owner,0.053329232554837125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Married'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_attribute_to_divide(df):\n",
    "    IG = []\n",
    "    attributes = df.keys()[:-1]\n",
    "    target = df.keys()[-1]\n",
    "    \n",
    "    for attribute in attributes:\n",
    "        ig = calcualte_entropy_of_dataset(df) - entropy_of_attribute(df,attribute)\n",
    "        print(f'{attribute},{ig}')\n",
    "        IG.append(calcualte_entropy_of_dataset(df) - entropy_of_attribute(df,attribute))\n",
    "        \n",
    "        \n",
    "    return attributes[np.argmax(IG)]\n",
    "\n",
    "best_attribute_to_divide(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0a357",
   "metadata": {},
   "source": [
    "# Decesion Trees for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f5bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_price = np.array([5.59, 9.54, 9.85, 4.15, 6.87, 9.83, 8.12, 8.61, 8.89, 8.92]).reshape(10, -1)\n",
    "km_driven = np.array([27000, 43000, 6900, 5200, 42450, 2071, 18796, 33429, 20273, 42376]).reshape(10, -1)\n",
    "age = np.array([8, 9, 5, 11, 8, 4, 7, 7, 6, 7]).reshape(10, -1)\n",
    "selling_price = np.array([3.35, 4.75, 7.25, 2.85, 4.6, 9.25, 6.75, 6.5, 8.75, 7.45]).reshape(10, -1)\n",
    "\n",
    "df = np.concatenate([present_price,km_driven,age,selling_price],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7f7a2",
   "metadata": {},
   "source": [
    "## Split the dataset according to the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941af1e2",
   "metadata": {},
   "source": [
    "We will split the dataset by the rows such that:\n",
    "\n",
    "For every row we will consider a particular index. If the value of that feature in a particular row is greater than the threshold then we will put it on the right else on the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9420f201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([5.59e+00, 2.70e+04, 8.00e+00, 3.35e+00]),\n",
       "  array([9.54e+00, 4.30e+04, 9.00e+00, 4.75e+00]),\n",
       "  array([4.15e+00, 5.20e+03, 1.10e+01, 2.85e+00]),\n",
       "  array([6.870e+00, 4.245e+04, 8.000e+00, 4.600e+00]),\n",
       "  array([8.1200e+00, 1.8796e+04, 7.0000e+00, 6.7500e+00]),\n",
       "  array([8.6100e+00, 3.3429e+04, 7.0000e+00, 6.5000e+00]),\n",
       "  array([8.9200e+00, 4.2376e+04, 7.0000e+00, 7.4500e+00])],\n",
       " [array([9.85e+00, 6.90e+03, 5.00e+00, 7.25e+00]),\n",
       "  array([   9.83, 2071.  ,    4.  ,    9.25]),\n",
       "  array([8.8900e+00, 2.0273e+04, 6.0000e+00, 8.7500e+00])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(df,feature_index,threshold):\n",
    "    right = []\n",
    "    left = []\n",
    "    \n",
    "    for rows in df:\n",
    "        if rows[feature_index]>=threshold:\n",
    "            right.append(rows)\n",
    "            \n",
    "        else:\n",
    "            left.append(rows)\n",
    "            \n",
    "    return right,left\n",
    "\n",
    "split_dataset(df,2,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e147297",
   "metadata": {},
   "source": [
    "## Variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b90e87",
   "metadata": {},
   "source": [
    "We consider the reduction in variance that has happened with the split of the data sets by subtracting the weighted average of the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a81cc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_reduction(df,l_child,r_child):\n",
    "    weight_1 = len(l_child)/len(df)\n",
    "    weight_2 = len(r_child)/len(df)\n",
    "    \n",
    "    var_red = np.var(df) - (weight_1*np.var(l_child) + weight_2*np.var(r_child))\n",
    "    \n",
    "    return var_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0608133",
   "metadata": {},
   "source": [
    "## Get the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1289bf65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7716/1887177769.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbest_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mget_best_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7716/1887177769.py\u001b[0m in \u001b[0;36mget_best_split\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_right\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft__y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright__y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_left\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_right\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mcurr_var_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariance_reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft__y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright__y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "def get_best_split(dataset):\n",
    "    n_samples,n_features = dataset.shape\n",
    "    best_split={}\n",
    "    max_var_red = -float('inf')\n",
    "    \n",
    "    for feature_index in range(n_features):\n",
    "        features_values = dataset[:,feature_index]\n",
    "        possible_thresholds = np.unique(features_values)\n",
    "        \n",
    "        \n",
    "        for threshold in possible_thresholds:\n",
    "            dataset_left,dataset_right = split_dataset(dataset,feature_index,threshold)\n",
    "            \n",
    "            if(len(dataset_left)>0) and len(dataset_right)>0:\n",
    "                y,left__y,right__y = dataset[:,-1],dataset_left[:,-1],dataset_right[:,-1]\n",
    "                \n",
    "                curr_var_red = variance_reduction(y,left__y,right__y)\n",
    "                \n",
    "                if curr_var_red> max_var_red:\n",
    "                    best_split['feature_index'] = feature_index\n",
    "                    best_split['threshold'] = threshold\n",
    "                    best_split['dataset_left'] = dataset_left\n",
    "                    best_split['dataset_right'] = dataset_right\n",
    "                    best_split['var_reduced'] = curr_var_red\n",
    "                    \n",
    "                    max_var_red=curr_var_red\n",
    "                     \n",
    "    return best_split\n",
    "get_best_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed03e906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.35, 4.75, 7.25, 2.85, 4.6 , 9.25, 6.75, 6.5 , 8.75, 7.45])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef29bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
